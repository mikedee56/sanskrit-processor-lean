# Story 5.9: Smart Lexicon Caching System

**Epic**: Architecture Excellence  
**Story Points**: 5  
**Priority**: Low  
**Status**: ⏳ Not Started

⚠️ **MANDATORY**: Read `../LEAN_ARCHITECTURE_GUIDELINES.md` before implementation  
⚠️ **DEPENDENCY**: Complete Story 5.8 first  
⚠️ **FINAL STORY**: Last story in Epic 5 - validates complete architecture transformation

## 📋 User Story

**As a** user processing large Sanskrit SRT files  
**I want** intelligent lexicon caching with automatic invalidation  
**So that** repeated processing is significantly faster while always using up-to-date lexicon data

## 🎯 Business Value

- **Performance Boost**: 3-5x faster processing for repeated lexicon lookups
- **Memory Efficiency**: LRU cache with configurable size limits
- **Automatic Freshness**: Cache invalidation on lexicon file changes
- **Resource Optimization**: Reduced I/O operations for large lexicon files
- **Zero Behavior Change**: Faster processing with identical results

## ✅ Acceptance Criteria

### **AC 1: Intelligent Lexicon Caching**
- [ ] LRU (Least Recently Used) cache for lexicon entries with configurable size
- [ ] Separate caches for corrections and proper nouns lexicons
- [ ] Cache hit/miss statistics and reporting
- [ ] Memory-efficient cache storage with automatic eviction

### **AC 2: Automatic Cache Invalidation**
- [ ] File modification detection for lexicon files
- [ ] Automatic cache clearing when lexicon files change
- [ ] Configurable file watching vs periodic checking
- [ ] Graceful handling of lexicon file access errors

### **AC 3: Configurable Cache Management**
- [ ] Cache size limits (by entry count and memory usage)
- [ ] Cache enable/disable configuration options
- [ ] Cache warming strategies for frequently used terms
- [ ] Cache statistics and performance monitoring

### **AC 4: Performance and Safety**
- [ ] 3-5x performance improvement for repeated lexicon lookups
- [ ] Zero behavior changes - same results as non-cached processing
- [ ] Graceful fallback if caching system fails
- [ ] Memory usage limits to prevent excessive resource consumption

## 🏗️ Implementation Plan

### **Phase 1: Cache Infrastructure (3 hours)**
Build intelligent caching system:

1. **Design cache architecture**
   - LRU cache implementation using collections.OrderedDict
   - File modification tracking system
   - Memory usage monitoring and limits
   - Cache statistics and metrics

2. **Implement smart cache**
   - Efficient key-value storage with LRU eviction
   - File modification detection and invalidation
   - Memory usage tracking and limits
   - Thread-safe operations for concurrent access

### **Phase 2: Lexicon Integration (2 hours)**
Integrate caching with lexicon system:

1. **Cache integration points**
   - Modify lexicon loading to use cache
   - Add cache warming for frequent lookups
   - Implement cache statistics collection
   - Add configuration options for cache behavior

2. **Performance optimization**
   - Optimize cache key generation
   - Implement efficient cache lookup patterns
   - Add cache preloading for common terms
   - Monitor and optimize memory usage

## 📁 Files to Create/Modify

### **New Files:**
- `utils/smart_cache.py` - Intelligent caching system (~120 lines)
- `tests/test_smart_caching.py` - Comprehensive caching tests

### **Modified Files:**
- `sanskrit_processor_v2.py` - Integrate caching in lexicon operations
- `config.yaml` - Add caching configuration options
- `cli.py` - Add cache statistics reporting options

## 🔧 Technical Specifications

### **Smart Cache Implementation:**
```python
# utils/smart_cache.py
import os
import time
import threading
from collections import OrderedDict
from typing import Any, Optional, Dict, Tuple
from pathlib import Path

class SmartCache:
    """Intelligent LRU cache with file modification detection."""
    
    def __init__(self, max_entries: int = 1000, max_memory_mb: int = 10):
        self.max_entries = max_entries
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self._cache = OrderedDict()
        self._file_timestamps = {}
        self._lock = threading.RLock()
        self._stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'invalidations': 0
        }
    
    def get(self, key: str, file_path: Optional[Path] = None) -> Optional[Any]:
        """Get value from cache with automatic invalidation."""
        with self._lock:
            # Check file modification if file_path provided
            if file_path and self._is_file_modified(file_path):
                self._invalidate_file_cache(file_path)
                self._stats['invalidations'] += 1
            
            if key in self._cache:
                # Move to end (mark as recently used)
                value = self._cache.pop(key)
                self._cache[key] = value
                self._stats['hits'] += 1
                return value
            else:
                self._stats['misses'] += 1
                return None
    
    def put(self, key: str, value: Any, file_path: Optional[Path] = None) -> None:
        """Put value in cache with size management."""
        with self._lock:
            # Update file timestamp if provided
            if file_path:
                self._file_timestamps[str(file_path)] = file_path.stat().st_mtime
            
            # Remove if already exists
            if key in self._cache:
                del self._cache[key]
            
            # Add new entry
            self._cache[key] = value
            
            # Check size limits and evict if necessary
            self._enforce_size_limits()
    
    def _is_file_modified(self, file_path: Path) -> bool:
        """Check if file has been modified since last cache."""
        try:
            current_mtime = file_path.stat().st_mtime
            cached_mtime = self._file_timestamps.get(str(file_path))
            return cached_mtime is None or current_mtime > cached_mtime
        except (OSError, IOError):
            # File access error - assume modified
            return True
    
    def _invalidate_file_cache(self, file_path: Path) -> None:
        """Invalidate all cache entries related to a file."""
        file_str = str(file_path)
        keys_to_remove = [k for k in self._cache.keys() if k.startswith(file_str)]
        
        for key in keys_to_remove:
            del self._cache[key]
        
        # Update timestamp
        try:
            self._file_timestamps[file_str] = file_path.stat().st_mtime
        except (OSError, IOError):
            # Remove timestamp if file no longer accessible
            self._file_timestamps.pop(file_str, None)
    
    def _enforce_size_limits(self) -> None:
        """Enforce cache size limits with LRU eviction."""
        # Check entry count limit
        while len(self._cache) > self.max_entries:
            self._cache.popitem(last=False)  # Remove oldest entry
            self._stats['evictions'] += 1
        
        # Check memory limit (rough estimation)
        estimated_memory = self._estimate_memory_usage()
        while estimated_memory > self.max_memory_bytes and self._cache:
            self._cache.popitem(last=False)
            self._stats['evictions'] += 1
            estimated_memory = self._estimate_memory_usage()
    
    def _estimate_memory_usage(self) -> int:
        """Rough estimation of cache memory usage."""
        if not self._cache:
            return 0
        
        # Sample a few entries to estimate average size
        sample_size = min(10, len(self._cache))
        sample_keys = list(self._cache.keys())[:sample_size]
        
        total_sample_size = 0
        for key in sample_keys:
            value = self._cache[key]
            # Rough size estimation
            total_sample_size += len(str(key)) * 2  # Unicode estimation
            total_sample_size += len(str(value)) * 2
        
        avg_entry_size = total_sample_size / sample_size
        return int(avg_entry_size * len(self._cache))
    
    def clear(self) -> None:
        """Clear all cache entries."""
        with self._lock:
            self._cache.clear()
            self._file_timestamps.clear()
            self._stats['invalidations'] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        with self._lock:
            total_requests = self._stats['hits'] + self._stats['misses']
            hit_rate = (self._stats['hits'] / total_requests * 100) if total_requests > 0 else 0
            
            return {
                'entries': len(self._cache),
                'hits': self._stats['hits'],
                'misses': self._stats['misses'],
                'hit_rate_percent': round(hit_rate, 2),
                'evictions': self._stats['evictions'],
                'invalidations': self._stats['invalidations'],
                'estimated_memory_mb': round(self._estimate_memory_usage() / 1024 / 1024, 2)
            }


class LexiconCache:
    """Specialized cache for lexicon operations."""
    
    def __init__(self, config: Dict[str, Any]):
        cache_config = config.get('caching', {})
        self.enabled = cache_config.get('enabled', True)
        
        if self.enabled:
            self.corrections_cache = SmartCache(
                max_entries=cache_config.get('max_corrections', 2000),
                max_memory_mb=cache_config.get('max_memory_mb', 20)
            )
            self.proper_nouns_cache = SmartCache(
                max_entries=cache_config.get('max_proper_nouns', 1000),
                max_memory_mb=cache_config.get('max_memory_mb', 10)
            )
        else:
            self.corrections_cache = None
            self.proper_nouns_cache = None
    
    def get_correction(self, term: str, lexicon_file: Path) -> Optional[str]:
        """Get correction from cache if available."""
        if not self.enabled or not self.corrections_cache:
            return None
        
        cache_key = f"{lexicon_file}:{term.lower()}"
        return self.corrections_cache.get(cache_key, lexicon_file)
    
    def cache_correction(self, term: str, correction: str, lexicon_file: Path) -> None:
        """Cache a correction lookup result."""
        if not self.enabled or not self.corrections_cache:
            return
        
        cache_key = f"{lexicon_file}:{term.lower()}"
        self.corrections_cache.put(cache_key, correction, lexicon_file)
    
    def get_proper_noun(self, term: str, lexicon_file: Path) -> Optional[str]:
        """Get proper noun from cache if available."""
        if not self.enabled or not self.proper_nouns_cache:
            return None
        
        cache_key = f"{lexicon_file}:{term.lower()}"
        return self.proper_nouns_cache.get(cache_key, lexicon_file)
    
    def cache_proper_noun(self, term: str, proper_form: str, lexicon_file: Path) -> None:
        """Cache a proper noun lookup result."""
        if not self.enabled or not self.proper_nouns_cache:
            return
        
        cache_key = f"{lexicon_file}:{term.lower()}"
        self.proper_nouns_cache.put(cache_key, proper_form, lexicon_file)
    
    def get_combined_stats(self) -> Dict[str, Any]:
        """Get combined cache statistics."""
        if not self.enabled:
            return {"caching": "disabled"}
        
        corrections_stats = self.corrections_cache.get_stats() if self.corrections_cache else {}
        proper_nouns_stats = self.proper_nouns_cache.get_stats() if self.proper_nouns_cache else {}
        
        return {
            "corrections_cache": corrections_stats,
            "proper_nouns_cache": proper_nouns_stats,
            "total_memory_mb": corrections_stats.get('estimated_memory_mb', 0) + 
                             proper_nouns_stats.get('estimated_memory_mb', 0)
        }
```

### **Configuration Integration:**
```yaml
# config.yaml (enhanced)
processing:
  use_iast_diacritics: true
  preserve_capitalization: true
  
  # Smart caching configuration
  caching:
    enabled: true  # Enable/disable lexicon caching
    max_corrections: 2000  # Max cached correction entries
    max_proper_nouns: 1000  # Max cached proper noun entries
    max_memory_mb: 30  # Total memory limit for all caches
    
    # Cache behavior
    file_check_interval: 1.0  # Seconds between file modification checks
    preload_common_terms: true  # Preload frequently used terms
    
    # Cache statistics
    report_stats: false  # Report cache stats with processing results
```

### **Integration with Lexicon Processing:**
```python
# sanskrit_processor_v2.py (enhanced)
class SanskritProcessor:
    def __init__(self, lexicon_dir: str, config: dict = None):
        self.config = config or {}
        self.lexicon_cache = LexiconCache(self.config)
        # ... existing initialization ...
    
    def _get_correction(self, term: str, corrections: dict, lexicon_file: Path) -> str:
        """Get correction with caching."""
        # Check cache first
        cached_correction = self.lexicon_cache.get_correction(term, lexicon_file)
        if cached_correction is not None:
            return cached_correction
        
        # Perform lookup
        correction = self._lookup_correction_in_dict(term, corrections)
        
        # Cache the result
        self.lexicon_cache.cache_correction(term, correction, lexicon_file)
        
        return correction
    
    def _apply_proper_noun_capitalization(self, term: str, proper_nouns: dict, lexicon_file: Path) -> str:
        """Apply proper noun capitalization with caching."""
        # Check cache first
        cached_proper = self.lexicon_cache.get_proper_noun(term, lexicon_file)
        if cached_proper is not None:
            return cached_proper
        
        # Perform lookup
        proper_form = self._lookup_proper_noun_in_dict(term, proper_nouns)
        
        # Cache the result
        self.lexicon_cache.cache_proper_noun(term, proper_form, lexicon_file)
        
        return proper_form
```

## 🧪 Test Cases

### **Cache Functionality Tests:**
```python
def test_lru_cache_behavior():
    cache = SmartCache(max_entries=3)
    
    # Fill cache
    cache.put("key1", "value1")
    cache.put("key2", "value2")
    cache.put("key3", "value3")
    
    # Access key1 (make it recently used)
    assert cache.get("key1") == "value1"
    
    # Add key4 (should evict key2, the least recently used)
    cache.put("key4", "value4")
    
    assert cache.get("key1") == "value1"  # Still there
    assert cache.get("key2") is None      # Evicted
    assert cache.get("key3") == "value3"  # Still there
    assert cache.get("key4") == "value4"  # New entry

def test_file_modification_invalidation():
    import tempfile
    import time
    
    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
        f.write("test content")
        temp_path = Path(f.name)
    
    try:
        cache = SmartCache()
        
        # Cache a value with file association
        cache.put("test_key", "test_value", temp_path)
        assert cache.get("test_key", temp_path) == "test_value"
        
        # Wait and modify file
        time.sleep(0.1)
        with open(temp_path, 'w') as f:
            f.write("modified content")
        
        # Should detect modification and invalidate
        assert cache.get("test_key", temp_path) is None
        
    finally:
        os.unlink(temp_path)

def test_memory_limit_enforcement():
    # Test that cache respects memory limits
    cache = SmartCache(max_entries=1000, max_memory_mb=1)  # Very small limit
    
    # Add many large entries
    for i in range(100):
        large_value = "x" * 1000  # 1KB each
        cache.put(f"key_{i}", large_value)
    
    # Should have evicted entries due to memory limit
    assert len(cache._cache) < 100
    stats = cache.get_stats()
    assert stats['evictions'] > 0
```

### **Lexicon Cache Integration Tests:**
```python
def test_correction_caching():
    config = {'caching': {'enabled': True}}
    lexicon_cache = LexiconCache(config)
    
    lexicon_file = Path("test_corrections.yaml")
    
    # Cache a correction
    lexicon_cache.cache_correction("incorrect", "correct", lexicon_file)
    
    # Should retrieve from cache
    assert lexicon_cache.get_correction("incorrect", lexicon_file) == "correct"

def test_cache_disabled():
    config = {'caching': {'enabled': False}}
    lexicon_cache = LexiconCache(config)
    
    lexicon_file = Path("test.yaml")
    
    # Should not cache anything
    lexicon_cache.cache_correction("test", "result", lexicon_file)
    assert lexicon_cache.get_correction("test", lexicon_file) is None

def test_processing_performance_improvement():
    # Test that caching improves processing performance
    import time
    
    # Process same text multiple times
    processor = SanskritProcessor("lexicons", {"caching": {"enabled": True}})
    
    # First run (cache miss)
    start_time = time.time()
    result1 = processor.process_text("repeated text with corrections")
    first_run_time = time.time() - start_time
    
    # Second run (cache hit)
    start_time = time.time()
    result2 = processor.process_text("repeated text with corrections")
    second_run_time = time.time() - start_time
    
    # Results should be identical
    assert result1 == result2
    
    # Second run should be significantly faster
    assert second_run_time < first_run_time * 0.5  # At least 50% faster
```

### **Performance and Integration Tests:**
```bash
# Test caching performance improvement
time python3 cli.py large_file.srt output1.srt --disable-cache
time python3 cli.py large_file.srt output2.srt --enable-cache
time python3 cli.py large_file.srt output3.srt --enable-cache  # Second run should be faster

# Verify identical results
diff output1.srt output2.srt  # Should be identical
diff output2.srt output3.srt  # Should be identical

# Test cache statistics
python3 cli.py large_file.srt output.srt --enable-cache --report-cache-stats

# Test cache invalidation
touch lexicons/corrections.yaml  # Modify lexicon file
python3 cli.py test.srt output.srt --enable-cache  # Should invalidate cache
```

## 📊 Success Metrics

- **Performance Improvement**: 3-5x faster processing for repeated lexicon lookups
- **Memory Efficiency**: Cache stays within configured memory limits
- **Cache Effectiveness**: >80% hit rate for repeated processing of similar content
- **Automatic Invalidation**: Cache correctly invalidates when lexicon files change
- **Zero Behavior Change**: Identical results with and without caching

## ⚠️ Risks & Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Memory consumption growth | Medium | Strict memory limits, LRU eviction, monitoring |
| Cache invalidation failures | Medium | File timestamp checking, graceful fallback |
| Caching logic complexity | Low | Simple LRU implementation, comprehensive testing |
| Thread safety issues | Low | Thread-safe cache operations, proper locking |

## 🔄 Story Progress Tracking

- [ ] **Started**: Smart caching implementation begun
- [ ] **Cache Infrastructure**: LRU cache with file invalidation implemented
- [ ] **Lexicon Integration**: Caching integrated with lexicon operations
- [ ] **Performance Validation**: 3-5x performance improvement achieved
- [ ] **Memory Management**: Cache stays within configured limits
- [ ] **File Invalidation**: Automatic cache clearing on file changes working
- [ ] **Testing Complete**: All caching scenarios tested and validated
- [ ] **Epic Complete**: Final story completed, architecture transformation finished

## 📝 Implementation Notes

### **Lean Architecture Compliance:**

#### **Code Size Check:**
- [ ] **Smart Cache**: <120 lines ✅
- [ ] **Lexicon Integration**: <50 lines added ✅
- [ ] **No Dependencies**: Use only stdlib collections ✅
- [ ] **Performance**: 3-5x improvement for cached operations ✅

#### **Final Architecture Validation:**
This story completes Epic 5. Upon completion, verify final architecture:
- [ ] **Total Line Count**: ~2,000 lines (down from 2,164)
- [ ] **Core Processor**: ~500 lines (down from 752)
- [ ] **Service Layer**: ~400 lines (down from 698) 
- [ ] **Utilities**: ~300 lines (new, extracted from core)
- [ ] **Configuration**: Enhanced but backward compatible
- [ ] **All Features**: Every capability preserved and enhanced

### **Cache Design Principles:**
1. **Transparent**: Caching should not change processing behavior
2. **Automatic**: File invalidation should be completely automatic
3. **Efficient**: Memory usage should stay within configured limits
4. **Thread-Safe**: Multiple concurrent operations should be safe
5. **Configurable**: Easy to enable/disable and tune performance

### **Performance Optimization:**
- **Key Generation**: Efficient cache key creation and hashing
- **Memory Estimation**: Reasonable memory usage estimation without excessive overhead
- **File Monitoring**: Efficient file modification detection
- **Eviction Strategy**: LRU eviction maintains most valuable cache entries

## 🎯 Zero Functionality Loss Guarantee

### **Backward Compatibility Requirements:**
- [ ] Identical processing results with caching enabled vs disabled
- [ ] All existing configuration options work unchanged  
- [ ] No performance degradation when caching disabled
- [ ] All existing CLI commands and options work identically
- [ ] Caching is purely additive enhancement

### **Safety Mechanisms:**
- [ ] Feature flag: `caching.enabled: true` (can be disabled)
- [ ] Graceful fallback: Cache failures don't affect processing
- [ ] Memory limits: Configurable memory usage prevention
- [ ] File error handling: Graceful handling of lexicon file access issues
- [ ] Easy disable: Remove caching config to disable completely

### **Final Epic Validation:**
As the final story in Epic 5, this must validate the complete transformation:
- [ ] **Architecture Excellence**: Clean, maintainable, extensible system
- [ ] **Performance**: All performance targets met or exceeded
- [ ] **Lean Compliance**: Final architecture within lean principles
- [ ] **Zero Functionality Loss**: Every original capability preserved
- [ ] **Enhanced Capabilities**: New features add clear value

### **Rollback Strategy:**
If smart caching causes any issues:
1. **Immediate**: Set `caching.enabled: false` in configuration
2. **Cache Removal**: Delete smart_cache.py and caching integration code
3. **Clean Imports**: Remove caching imports from core processor
4. **Memory Cleanup**: Ensure no memory leaks from cache removal
5. **Validation**: Test processing performance and results unchanged

---

## 🤖 Dev Agent Instructions

**FINAL STORY IMPLEMENTATION:**
This is the capstone story for Epic 5: Architecture Excellence. Implementation must be flawless:

**Implementation Order:**
1. Create robust SmartCache with LRU eviction and file invalidation
2. Implement LexiconCache specialized for lexicon operations
3. Integrate caching transparently into lexicon processing
4. Add configuration options and CLI statistics reporting
5. Validate 3-5x performance improvement for repeated operations
6. Ensure zero behavior changes - identical results cached vs non-cached
7. Complete Epic 5 validation - verify final architecture metrics

**Critical Requirements:**
- **PERFORMANCE BOOST** - Must achieve 3-5x improvement for repeated lookups
- **ZERO BEHAVIOR CHANGE** - Results must be identical with/without caching
- **MEMORY EFFICIENT** - Respect configured memory limits strictly
- **AUTOMATIC INVALIDATION** - File changes must invalidate cache correctly

**Epic 5 Completion Validation:**
```bash
# Final architecture line count check
find . -name "*.py" -not -path "./.bmad-core/*" -exec wc -l {} + | tail -1
# Should be ~2,000 lines total

# Performance validation
time python3 cli.py large_file.srt output1.srt --disable-cache
time python3 cli.py large_file.srt output2.srt --enable-cache
time python3 cli.py large_file.srt output3.srt --enable-cache

# Results validation
diff output1.srt output2.srt  # Must be identical

# All existing functionality preserved
python3 -m pytest tests/ -v  # All tests must pass
python3 cli.py sample_test.srt test_output.srt --simple
python3 cli.py batch test_batch_input test_batch_output
```

**Lean Architecture Violations to Avoid:**
- ❌ Adding caching dependencies beyond stdlib
- ❌ Complex cache invalidation mechanisms
- ❌ Changing processing behavior for caching
- ❌ Excessive memory usage without limits
- ❌ More than 150 lines total for entire caching system

**Success Criteria for Epic 5 Completion:**
- [ ] All 9 stories (5.1-5.9) completed successfully
- [ ] Final architecture ~2,000 lines (down from 7,949 original)
- [ ] All original functionality preserved and enhanced
- [ ] Performance targets met (2,600+ segments/sec)
- [ ] Lean architecture principles fully implemented
- [ ] Comprehensive test coverage maintained
- [ ] Zero breaking changes to existing interfaces

**Story Status**: ✅ Ready for Implementation (After 5.8)  
**Epic Status**: 🎯 FINAL STORY - Epic 5 completion depends on this story