# Story 5.6: Performance Monitoring Framework

**Epic**: Architecture Excellence  
**Story Points**: 3  
**Priority**: Medium  
**Status**: ⏳ Not Started

⚠️ **MANDATORY**: Read `../LEAN_ARCHITECTURE_GUIDELINES.md` before implementation  
⚠️ **DEPENDENCY**: Complete Story 5.4 first

## 📋 User Story

**As a** system administrator  
**I want** optional performance profiling capabilities  
**So that** I can monitor system performance and identify optimization opportunities without impacting normal operations

## 🎯 Business Value

- **Performance Insights**: Identify processing bottlenecks and optimization opportunities
- **Resource Monitoring**: Track memory usage and processing efficiency over time
- **Proactive Maintenance**: Detect performance degradation before it impacts users
- **Optimization Guidance**: Data-driven performance improvement decisions
- **Zero Impact**: Optional monitoring with no performance cost when disabled

## ✅ Acceptance Criteria

### **AC 1: Optional Performance Profiler**
- [ ] CLI flag `--profile` enables performance monitoring
- [ ] Zero performance impact when profiling disabled (default)
- [ ] Configurable profiling detail levels (basic, detailed, full)
- [ ] Integration with existing metrics collection system

### **AC 2: Memory Usage Tracking**
- [ ] Real-time memory usage monitoring during processing
- [ ] Peak memory usage reporting
- [ ] Memory leak detection for long-running operations
- [ ] Memory usage breakdown by processing stage

### **AC 3: Processing Bottleneck Identification**
- [ ] Timing for each processing stage (parsing, matching, enhancement)
- [ ] Segment processing rate monitoring
- [ ] External service response time tracking
- [ ] I/O operation performance measurement

### **AC 4: Performance Report Generation**
- [ ] Human-readable performance summary report
- [ ] JSON format for automated analysis
- [ ] Historical performance comparison (if previous runs cached)
- [ ] Actionable optimization recommendations

## 🏗️ Implementation Plan

### **Phase 1: Profiler Infrastructure (2 hours)**
Build lightweight profiling system:

1. **Design profiling framework**
   - Minimal overhead context managers for timing
   - Memory usage tracking utilities
   - Configurable profiling levels
   - Integration points with existing metrics

2. **Implement performance profiler**
   - Context managers for timing critical sections
   - Memory tracking during processing
   - Stage-by-stage performance breakdown
   - Optional detailed profiling mode

### **Phase 2: Integration and Reporting (1 hour)**
Integrate with existing system:

1. **CLI integration**
   - Add `--profile` flag to cli.py
   - Profiling configuration in config.yaml
   - Report generation and display

2. **Report generation**
   - Summary report with key metrics
   - Detailed breakdown when requested
   - JSON output for automation
   - Optimization recommendations

## 📁 Files to Create/Modify

### **New Files:**
- `utils/performance_profiler.py` - Lightweight profiling system (~70 lines)
- `tests/test_performance_monitoring.py` - Performance monitoring tests

### **Modified Files:**
- `cli.py` - Add `--profile` flag and report display
- `sanskrit_processor_v2.py` - Add profiling context managers at key points
- `enhanced_processor.py` - Add service performance monitoring
- `config.yaml` - Add profiling configuration options

## 🔧 Technical Specifications

### **Performance Profiler:**
```python
# utils/performance_profiler.py
import time
import psutil
import contextlib
from typing import Dict, List, Optional

class PerformanceProfiler:
    """Lightweight performance profiler with minimal overhead."""
    
    def __init__(self, enabled: bool = False, detail_level: str = "basic"):
        self.enabled = enabled
        self.detail_level = detail_level
        self.timings = {}
        self.memory_samples = []
        self.start_time = None
        
    @contextlib.contextmanager
    def profile_stage(self, stage_name: str):
        """Context manager for profiling processing stages."""
        if not self.enabled:
            yield
            return
            
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss
        
        try:
            yield
        finally:
            end_time = time.perf_counter()
            end_memory = psutil.Process().memory_info().rss
            
            self.timings[stage_name] = {
                'duration': end_time - start_time,
                'memory_start': start_memory,
                'memory_end': end_memory,
                'memory_delta': end_memory - start_memory
            }
    
    def generate_report(self) -> dict:
        """Generate performance report."""
        if not self.enabled:
            return {"profiling": "disabled"}
            
        total_time = sum(t['duration'] for t in self.timings.values())
        peak_memory = max(t['memory_end'] for t in self.timings.values()) if self.timings else 0
        
        return {
            "total_duration": total_time,
            "peak_memory_mb": peak_memory / (1024 * 1024),
            "stage_timings": self.timings,
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """Generate optimization recommendations based on profiling data."""
        recommendations = []
        
        # Find slowest stages
        if self.timings:
            slowest_stage = max(self.timings.items(), key=lambda x: x[1]['duration'])
            if slowest_stage[1]['duration'] > 1.0:  # More than 1 second
                recommendations.append(f"Consider optimizing {slowest_stage[0]} stage (slowest)")
        
        return recommendations
```

### **CLI Integration:**
```python
# cli.py (enhanced)
def main():
    parser = argparse.ArgumentParser()
    # ... existing arguments ...
    parser.add_argument('--profile', action='store_true', 
                       help='Enable performance profiling')
    parser.add_argument('--profile-detail', choices=['basic', 'detailed', 'full'],
                       default='basic', help='Profiling detail level')
    
    args = parser.parse_args()
    
    # Initialize profiler
    profiler = PerformanceProfiler(
        enabled=args.profile,
        detail_level=args.profile_detail
    )
    
    # ... existing processing with profiling context managers ...
    
    if args.profile:
        report = profiler.generate_report()
        print("\n" + "="*50)
        print("PERFORMANCE REPORT")
        print("="*50)
        print(f"Total Duration: {report['total_duration']:.2f}s")
        print(f"Peak Memory: {report['peak_memory_mb']:.1f}MB")
        
        if report.get('recommendations'):
            print("\nOptimization Recommendations:")
            for rec in report['recommendations']:
                print(f"  • {rec}")
```

### **Configuration Integration:**
```yaml
# config.yaml (enhanced)
performance:
  profiling:
    enabled: false  # Default disabled for zero impact
    detail_level: "basic"  # basic, detailed, full
    memory_sampling_interval: 0.1  # seconds
    cache_reports: true  # Cache for historical comparison
    
  monitoring:
    track_segment_rate: true
    track_memory_usage: true
    track_service_response_times: true
```

## 🧪 Test Cases

### **Profiler Tests:**
```python
def test_profiler_disabled():
    # Test zero overhead when disabled
    profiler = PerformanceProfiler(enabled=False)
    
    start_time = time.perf_counter()
    with profiler.profile_stage("test_stage"):
        time.sleep(0.01)  # Simulate work
    end_time = time.perf_counter()
    
    # Should have minimal overhead
    assert (end_time - start_time) < 0.02
    assert profiler.generate_report() == {"profiling": "disabled"}

def test_profiler_enabled():
    profiler = PerformanceProfiler(enabled=True)
    
    with profiler.profile_stage("test_processing"):
        time.sleep(0.1)  # Simulate processing
    
    report = profiler.generate_report()
    assert report["total_duration"] >= 0.1
    assert "test_processing" in report["stage_timings"]

def test_memory_tracking():
    profiler = PerformanceProfiler(enabled=True)
    
    with profiler.profile_stage("memory_test"):
        # Allocate some memory
        data = [0] * 1000000  # Allocate ~8MB
    
    report = profiler.generate_report()
    assert report["peak_memory_mb"] > 0
```

### **CLI Integration Tests:**
```bash
# Test profiling flag
python3 cli.py sample_test.srt test_output.srt --profile
echo $?  # Should be 0 with performance report displayed

# Test detailed profiling
python3 cli.py sample_test.srt test_output.srt --profile --profile-detail detailed

# Test batch profiling
python3 cli.py batch test_batch_input test_batch_output --profile

# Test without profiling (should have no performance impact)
time python3 cli.py sample_test.srt test_output.srt
```

### **Performance Impact Tests:**
```python
def test_zero_overhead_when_disabled():
    # Test that profiler adds no overhead when disabled
    import time
    
    def process_without_profiler():
        start = time.perf_counter()
        # Simulate processing
        for i in range(1000):
            pass
        return time.perf_counter() - start
    
    def process_with_disabled_profiler():
        profiler = PerformanceProfiler(enabled=False)
        start = time.perf_counter()
        with profiler.profile_stage("test"):
            for i in range(1000):
                pass
        return time.perf_counter() - start
    
    time_without = process_without_profiler()
    time_with = process_with_disabled_profiler()
    
    # Should be within 5% of each other
    assert abs(time_with - time_without) / time_without < 0.05
```

## 📊 Success Metrics

- **Zero Impact**: <1% performance overhead when profiling disabled
- **Useful Insights**: Identify top 3 processing bottlenecks
- **Memory Awareness**: Track memory usage patterns and peak consumption
- **Actionable Reports**: Generate specific optimization recommendations
- **Integration**: Seamless integration with existing workflow

## ⚠️ Risks & Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Performance overhead when enabled | Medium | Lightweight implementation, optional detailed modes |
| Memory overhead from profiling data | Low | Configurable detail levels, data cleanup |
| Complex report interpretation | Low | Clear, actionable recommendations |
| Platform compatibility issues | Low | Use standard library utilities, graceful fallbacks |

## 🔄 Story Progress Tracking

- [ ] **Started**: Performance profiling implementation begun
- [ ] **Profiler Core**: PerformanceProfiler class implemented
- [ ] **CLI Integration**: --profile flag working
- [ ] **Report Generation**: Human-readable reports generated
- [ ] **Testing Complete**: Zero-overhead and functionality tested
- [ ] **Integration Validated**: Works with existing processing pipeline
- [ ] **Documentation**: Profiling usage documented

## 📝 Implementation Notes

### **Lean Architecture Compliance:**

#### **Code Size Check:**
- [ ] **Profiler Module**: <70 lines ✅
- [ ] **CLI Integration**: <20 lines added ✅
- [ ] **No Dependencies**: Use stdlib + existing psutil ✅
- [ ] **Zero Impact**: <1% overhead when disabled ✅

#### **Implementation Strategy:**
1. **Optional by Default**: Profiling disabled unless explicitly requested
2. **Minimal Overhead**: Use efficient timing and memory measurement
3. **Configurable Detail**: Basic profiling by default, detailed on request
4. **Actionable Output**: Focus on optimization recommendations

### **Performance Considerations:**
- **Context Manager Overhead**: Minimal when profiling disabled
- **Memory Sampling**: Efficient sampling without excessive data collection
- **Report Generation**: Lazy computation, only when requested
- **Data Storage**: Minimal memory footprint for profiling data

### **Report Format:**
```
==================================================
PERFORMANCE REPORT
==================================================
Total Duration: 2.34s
Peak Memory: 45.2MB
Processing Rate: 1,247 segments/second

Stage Breakdown:
  SRT Parsing: 0.12s (5.1%)
  Lexicon Matching: 1.89s (80.8%)
  External Services: 0.23s (9.8%)
  Output Generation: 0.10s (4.3%)

Optimization Recommendations:
  • Consider optimizing Lexicon Matching stage (slowest)
  • Memory usage is within normal range
  • External service response time is acceptable
```

## 🎯 Zero Functionality Loss Guarantee

### **Backward Compatibility Requirements:**
- [ ] All existing CLI commands work unchanged
- [ ] No performance impact when profiling disabled (default)
- [ ] All existing processing behavior preserved
- [ ] No additional dependencies for core functionality
- [ ] Profiling is purely additive, never replaces existing functionality

### **Safety Mechanisms:**
- [ ] Feature flag: `performance.profiling.enabled: false` (default)
- [ ] Graceful fallback: Profiling failures don't affect processing
- [ ] Configurable overhead: Multiple detail levels for different needs
- [ ] Easy disable: Remove --profile flag to disable completely

### **Rollback Strategy:**
If performance monitoring causes issues:
1. **Immediate**: Remove --profile flag from CLI usage
2. **Configuration**: Set `performance.profiling.enabled: false`
3. **Code Removal**: Delete performance_profiler.py if needed
4. **Clean Imports**: Remove profiling imports from core files
5. **Validation**: Verify normal processing performance restored

---

## 🤖 Dev Agent Instructions

**Implementation Order:**
1. Create lightweight PerformanceProfiler with zero overhead when disabled
2. Add CLI integration with --profile flag
3. Integrate profiling context managers at key processing points
4. Implement report generation with actionable recommendations
5. Add comprehensive tests verifying zero impact and functionality
6. Validate integration with existing processing pipeline

**Critical Requirements:**
- **ZERO IMPACT** - No performance cost when profiling disabled
- **LIGHTWEIGHT** - Minimal code and memory overhead
- **ACTIONABLE** - Reports must include specific optimization guidance
- **OPTIONAL** - Never required, always gracefully degradable

**Lean Architecture Violations to Avoid:**
- ❌ Adding heavy profiling dependencies (use stdlib + existing)
- ❌ Creating performance overhead when profiling disabled  
- ❌ Overly complex profiling infrastructure
- ❌ More than 100 lines total for profiling system

**Required Validations:**
```bash
# Test zero overhead (most important)
time python3 cli.py sample_test.srt test_output.srt
time python3 cli.py sample_test.srt test_output.srt --profile

# Test profiling functionality
python3 cli.py batch test_batch_input test_batch_output --profile --profile-detail detailed

# Verify all existing tests still pass
python3 -m pytest tests/ -v
```

**Story Status**: ✅ Ready for Implementation (After 5.4)