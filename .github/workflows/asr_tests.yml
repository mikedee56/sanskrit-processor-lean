# ASR Testing Suite CI/CD Pipeline
# Comprehensive testing for Sanskrit ASR correction functionality

name: ASR Testing Suite

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run nightly performance regression tests
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.9'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-benchmark pytest-mock
      
      - name: Run unit tests with coverage
        run: |
          pytest tests/unit/ -v \
            --cov=. \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --cov-fail-under=85
      
      - name: Upload coverage to Codecov
        if: success()
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unit-tests
          name: codecov-umbrella
      
      - name: Archive coverage HTML report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: coverage-report
          path: htmlcov/

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-mock
      
      - name: Generate test data
        run: |
          python tests/utils/test_data_generator.py
      
      - name: Run integration tests
        run: |
          pytest tests/integration/ -v \
            --tb=short
      
      - name: Test CLI integration
        run: |
          python cli.py --help
          python cli.py tests/fixtures/sample_asr_files/small_lecture.srt /tmp/test_output.srt --simple --verbose
          
      - name: Validate output files
        run: |
          test -f /tmp/test_output.srt
          echo "Integration test output file created successfully"

  acceptance-tests:
    name: Acceptance Tests (Golden Dataset)
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
      
      - name: Run golden dataset tests
        run: |
          pytest tests/acceptance/test_golden_dataset.py -v \
            --tb=short \
            -x  # Stop on first failure for faster feedback
      
      - name: Archive test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: acceptance-test-results
          path: |
            tests/fixtures/
            test_output*.srt

  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-benchmark
      
      - name: Run performance benchmarks
        run: |
          pytest tests/performance/ -v \
            --benchmark-only \
            --benchmark-json=benchmark_results.json \
            --benchmark-min-rounds=3
      
      - name: Check performance regression
        run: |
          python -c "
          import json
          import sys
          
          # Load benchmark results
          with open('benchmark_results.json') as f:
              results = json.load(f)
          
          # Check if any benchmark failed performance thresholds
          failed_benchmarks = []
          for benchmark in results.get('benchmarks', []):
              name = benchmark['name']
              stats = benchmark['stats']
              mean_time = stats['mean']
              
              # Define performance thresholds (seconds)
              thresholds = {
                  'small': 2.0,
                  'medium': 5.0, 
                  'large': 10.0
              }
              
              for size, threshold in thresholds.items():
                  if size in name.lower() and mean_time > threshold:
                      failed_benchmarks.append(f'{name}: {mean_time:.2f}s > {threshold}s')
          
          if failed_benchmarks:
              print('Performance regression detected:')
              for failure in failed_benchmarks:
                  print(f'  - {failure}')
              sys.exit(1)
          else:
              print('All performance benchmarks passed!')
          "
      
      - name: Archive benchmark results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark_results.json

  cross-platform-tests:
    name: Cross-Platform Compatibility
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11']
        exclude:
          # Skip some combinations to reduce CI time
          - os: macos-latest
            python-version: '3.8'
          - os: windows-latest  
            python-version: '3.8'
    
    runs-on: ${{ matrix.os }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
      
      - name: Test basic functionality
        run: |
          pytest tests/unit/test_asr_processor.py::TestSanskritProcessor::test_process_single_segment -v
      
      - name: Test CLI basic operation
        shell: bash
        run: |
          python cli.py --help
          echo "CLI help command works on ${{ matrix.os }}"

  regression-tests:
    name: Regression Tests  
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' # Only run on nightly schedule
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-benchmark
      
      - name: Generate comprehensive test data
        run: |
          python tests/utils/test_data_generator.py
      
      - name: Run full test suite
        run: |
          pytest tests/ -v \
            --tb=short \
            --maxfail=10 \
            --benchmark-skip  # Skip benchmarks in regression run
      
      - name: Run performance regression check
        run: |
          pytest tests/performance/test_speed_benchmarks.py::TestRegressionDetection -v
      
      - name: Archive comprehensive results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: regression-test-results
          path: |
            tests/fixtures/generated/
            test_output*.srt

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, acceptance-tests, performance-tests]
    if: always()
    
    steps:
      - name: Test Summary
        run: |
          echo "## ASR Testing Suite Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Acceptance Tests | ${{ needs.acceptance-tests.result }} |" >> $GITHUB_STEP_SUMMARY  
          echo "| Performance Tests | ${{ needs.performance-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.unit-tests.result }}" == "success" && \
                "${{ needs.integration-tests.result }}" == "success" && \
                "${{ needs.acceptance-tests.result }}" == "success" && \
                "${{ needs.performance-tests.result }}" == "success" ]]; then
            echo "✅ **All tests passed!** The ASR testing framework is working correctly." >> $GITHUB_STEP_SUMMARY
            exit 0
          else
            echo "❌ **Some tests failed.** Please check the individual test results." >> $GITHUB_STEP_SUMMARY
            exit 1
          fi